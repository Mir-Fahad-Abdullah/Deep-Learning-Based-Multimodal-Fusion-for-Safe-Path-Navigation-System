{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e062bb83-963b-4dac-be95-a64309079a28",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a9ba59-fe8c-4f64-846d-1764c5090a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604d630-2c0f-4940-b662-0ce59039a9fe",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56baf1b-869c-496a-b830-49e666b8ce30",
   "metadata": {},
   "source": [
    "### Directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f2f485-22b2-4556-86b8-00a2dd6c99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\ACER\\Jupyter_Notebook_Workplace\\Object-detection-dataset'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "valid_dir = os.path.join(base_dir, 'valid')\n",
    "output_label_map_path = os.path.join(base_dir, 'label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d161f-c8f6-4887-902f-f4650ea53c54",
   "metadata": {},
   "source": [
    "### Extracting Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487dafdc-d125-48cb-86ce-87135c3b405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels used in your annotations:\n",
      "1: Tom Tom\n",
      "2: bench\n",
      "3: bin\n",
      "4: bus\n",
      "5: car\n",
      "6: chair\n",
      "7: cng\n",
      "8: dog\n",
      "9: door\n",
      "10: glass partition\n",
      "11: motorcycle\n",
      "12: person\n",
      "13: pillar\n",
      "14: railing\n",
      "15: rickshaw\n",
      "16: shelf\n",
      "17: stair\n",
      "18: table\n",
      "19: tempu\n",
      "20: tom tom\n",
      "21: tree\n",
      "22: truck\n",
      "23: umbrella\n",
      "24: van\n",
      "\n",
      "Label map saved to: C:\\Users\\ACER\\Jupyter_Notebook_Workplace\\Object-detection-dataset\\label_map.pbtxt\n"
     ]
    }
   ],
   "source": [
    "# Extract labels from XML files\n",
    "def extract_labels(directory):\n",
    "    labels = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            tree = ET.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            for obj in root.findall('object'):\n",
    "                labels.add(obj.find('name').text.strip())\n",
    "    return labels\n",
    "\n",
    "# Extract and merge all labels\n",
    "train_labels = extract_labels(train_dir)\n",
    "valid_labels = extract_labels(valid_dir)\n",
    "all_labels = sorted(train_labels.union(valid_labels))\n",
    "\n",
    "# Create label_map dictionary\n",
    "label_map_dict = {i + 1: name for i, name in enumerate(all_labels)}\n",
    "\n",
    "# Print labels to console\n",
    "print(\"Labels used in your annotations:\")\n",
    "for idx, name in label_map_dict.items():\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Save to label_map.pbtxt\n",
    "def save_label_map(label_map, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for idx, name in label_map.items():\n",
    "            f.write(\"item {\\n\")\n",
    "            f.write(f\"  id: {idx}\\n\")\n",
    "            f.write(f\"  name: '{name}'\\n\")\n",
    "            f.write(\"}\\n\\n\")\n",
    "\n",
    "save_label_map(label_map_dict, output_label_map_path)\n",
    "\n",
    "print(f\"\\nLabel map saved to: {output_label_map_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2f15d-20c4-4c5d-946e-0d2abec631e4",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0edb72-9e21-4877-8cc4-08b500661ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, bbox):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "    return image, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77039250-f450-4ccd-8760-c70267b36880",
   "metadata": {},
   "source": [
    "### Apply and Save Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1242dd61-0c4c-4095-91c5-24fbe3373300",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dir = os.path.join(base_dir, 'train_augmented')\n",
    "os.makedirs(aug_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(train_dir):\n",
    "    if file.endswith('.jpg') or file.endswith('.png'):\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "        img_path = os.path.join(train_dir, file)\n",
    "        xml_path = os.path.join(train_dir, base_name + '.xml')\n",
    "\n",
    "        if not os.path.exists(xml_path):\n",
    "            continue  # Skip if annotation missing\n",
    "\n",
    "        # Load and augment image\n",
    "        image = tf.io.read_file(img_path)\n",
    "        image = tf.image.decode_jpeg(image, channels = 3)\n",
    "        image = tf.image.resize(image, (448, 448))\n",
    "        image = tf.cast(image, tf.uint8)\n",
    "\n",
    "        aug_image, _ = augment_image(image, None)\n",
    "        aug_image = tf.image.encode_jpeg(tf.cast(aug_image, tf.uint8))\n",
    "\n",
    "        # Save augmented image\n",
    "        aug_img_name = base_name + '_aug.jpg'\n",
    "        aug_img_path = os.path.join(aug_dir, aug_img_name)\n",
    "        tf.io.write_file(aug_img_path, aug_image)\n",
    "\n",
    "        # Copy original XML with new name\n",
    "        aug_xml_name = base_name + '_aug.xml'\n",
    "        shutil.copy(xml_path, os.path.join(aug_dir, aug_xml_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9faa6-e43e-45a2-b381-589a5ce8109d",
   "metadata": {},
   "source": [
    "### Merge Original and Augmented Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5416266c-436d-4c48-a249-94d0ef4c1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_dir = os.path.join(base_dir, 'train_combined')\n",
    "os.makedirs(train_combined_dir, exist_ok = True)\n",
    "\n",
    "for src_dir in [train_dir, aug_dir]:\n",
    "    for f in os.listdir(src_dir):\n",
    "        shutil.copy(os.path.join(src_dir, f), os.path.join(train_combined_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08848a7-90c2-487f-b529-8999fd83fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
